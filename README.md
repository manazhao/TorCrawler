
# TorCrawler
TorCrawler is single machine based web page crawler. It aims to be simple, reliable, fexlible and efficient.
## Highlights
### Simple
Most of it is coded by Perl, hence the code is concise. The core function only includes three files, Fetcher.pl, PageParser.pl and TorGet.php, resulting less than 400 total number of lines. 

### Reliable
All intermediate files are kept under a single folder. The cralwer threads work jointly in a map-reduce manner. As individual crawler agent is realized by process, crash of agents does not suspend the main program and the unfinished work will be reassigned in the coming round. The whole program is rather robust - it can be stopped and resumed any time and the results will stay intact.

### Flexible
It's higly customizable to your task. This means, first, you can specify the link following rules simply by a regular expression based file; second, you can plugin your own parser to extract information from the crawled html content. There is a Perl parser template you can follow. 

### Efficient
With Tor proxy technique, it can crawl urls with concurrent threads from multiple distinct IP addresses and more importantly, all this can be done on a single machine with a single IP address. Tor proxy helps unleash all your bandwidth and dismiss your concern of crawling speed limited by single IP. With tor, you can literally own as many virtual machines as you want, simply by a command line argument! The crawler scores each proxy based on the downloading speed and error rate and works with top ranked proxy candidates over time.  

## Installation
### Dependency
1. Multi-TOR, script that boots multiple Tor proxies on your machine. It can be downloaded from  https://github.com/jseidl/Multi-TOR. It should be fairly straightfoward to use it by following the instruction.

2. Perl and PHP language support and their related packages

  2.1 Perl 
  
  Getopt::Long, Data::Dumper, Digest:MD5, FindBin, File::Basename, Try::Tiny, JSON, pQuery, HTML::LinkExtractor. Install them through cpan
  
  2.2 PHP
  
  curl
  
## Usage
1. ### Start SOCKS proxy
Go to *Multi-Tor* directory and run script *multi-tor.sh* with the number of proxies as the only argument. If successful, you should see from the dump messages that there are processes created and listening to ports from 9050.

2. ### Run crawler

Go to the root folder of *TorCrawler* where you should see the following sub-folders,
* common - scripts independent with crawling task
* task - task specific files, each task corresponds to sub-folder

#### common
- **TorGet.php** 
  This script takes urls from standard input and crawl each url in the order it is fed. Below is an example usage:

  *TorGet.php -h127.0.0.1 -p9050 -d./ -w3*
    * -h127.0.0.1 - The proxy host (localhost most times)
    * -p9050  - proxy port starting from 9050 by default
    * -d./  - working directory, current folder in this example
    * -w3 - number of seconds of waiting between two consecutive http request
  
  If the crawling is successful, the html page will be saved under the working directory with the **md5 hash** of the original url as file name. At the same time, the program will output the crawling status of the given url in the following format:
  
    http://an.example.url *STATUS CODE*
    
    where *STATUS CODE* could be any of: *DL_OK*, *DL_ERR*. The meaning of the status should be self-explanative.
    The url and status code is separated by \t.
  
- **Fetcher.pl**
  
This is the driving script for crawling task. A example usage is,
  *Fetcher.pl --work-dir=./*
  which uses current directory as working directory. Working directory is the place where all crawling and parsing activities take place. The intermediate files and downloaded web pages will be saved here. Two files are mainly involed in  **Fetcher.pl**
  1. *url.status*
  
    Each line has two tab separated fields which are the targeting url and its status. This file will be updated when the crawling task proceeds and the status of each url will also be udpated accordingly. Possible status values include: 
    - *DL_NEW* - url that has not been crawled yet. It could be given by user or links extracted from crawled pages
    - *DL_ERR1* - url failing once
    - *DL_ERR2* - url failing twice
    - *DL_ERR3* - url failing three times
    - *DL_OK* - url that is crawled properly and it will be skipped in further crawling rounds, so is url that fails more than three times; otherwise, the url will be included in *url.in* file. 
  
  You should provide your *seeding urls* in *url.status* file and the status ought to be *DL_NEW*
  
  2. *url.in*

    Each line is a targeting url to crawl. This file will be updated after each fetching round. You do NOT need to provide this file as it is generated by the program. 

- **PageParser.pl**

  It parses the downloaded html files with the parser module you give. An example usage:
  
  *PageParser.pl --work-dir=. --parser-mod=task::zillow::PageParser < â€œyour files" *
  
  You are encouraged to place your parser under task with separate sub-folder as name space, like the above example. 
  
  The html files are read from standard input, one line per file. The parsing result file will be saved as json files under your working directory; parsing status will be dumped to standard output.
  
  An example *parser module* can be found at *common/PageParserTemplate.pm*. You are supposed to define your own parsing routines given the url pattern represented by regular expression. 
  
## Contributing

1. Fork it!
2. Create your feature branch: `git checkout -b my-new-feature`
3. Commit your changes: `git commit -am 'Add some feature'`
4. Push to the branch: `git push origin my-new-feature`
5. Submit a pull request :D

## History
1. 10-24-2015, initial import
## Credits

1. Qi Zhao, manazhao@gmail.com

## License

TODO: Write license
